{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data, convert strings to class labels\n",
    "note for loading data: uncomment tfidf data codes and comment out count data codes to switch between data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "# train_data = open(\"train_tfidf.csv\",'r').readlines()\n",
    "# dev_data = open(\"dev_tfidf.csv\",'r').readlines()\n",
    "# test_data = open(\"test_tfidf.csv\",'r').readlines()\n",
    "# vocab_data = open(\"vocab.txt\",'r').readlines()\n",
    "\n",
    "train_data = open(\"train_count.csv\",'r').readlines()\n",
    "dev_data = open(\"dev_count.csv\",'r').readlines()\n",
    "test_data = open(\"test_count.csv\",'r').readlines()\n",
    "vocab_data = open(\"vocab.txt\",'r').readlines()\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "x_dev = []\n",
    "y_dev = []\n",
    "x_test = []\n",
    "y_test = []\n",
    "tuple_list = []\n",
    "tfidf_list = []\n",
    "label_id = []\n",
    "tuple_id = []\n",
    "tuple_tfidf = []\n",
    "dev_tuple_id = []\n",
    "dev_tuple_tfidf = []\n",
    "dev_tuple_list = []\n",
    "vocab_list = []\n",
    "\n",
    "target = {91:None, 93:None, 34:None, 40:None, 41:None}\n",
    "def convert_feature(raw):\n",
    "    if raw == \"neg\": return 2\n",
    "    elif raw == \"neu\": return 1\n",
    "    elif raw == \"pos\": return 0\n",
    "\n",
    "for instance in vocab_data:\n",
    "    instance = instance.strip().split('\\t')\n",
    "    vocab_list.append(instance)\n",
    "\n",
    "#train data\n",
    "for instance in train_data[1:]:\n",
    "    instance = str(instance).translate(target)\n",
    "    instance = instance.strip().split(',')\n",
    "    tuple_id = list(instance[2::2])\n",
    "    tuple_tfidf = list(instance[3::2])\n",
    "    tuple_list.append(list(zip(tuple_id, tuple_tfidf)))\n",
    "    y_train.append(convert_feature(instance[0]))\n",
    "# tuple_array = np.array(tuple_list, dtype = object)\n",
    "# y_train = np.array(y_train, dtype = float)\n",
    "\n",
    "#dev data\n",
    "for instance in dev_data[1:]:\n",
    "    instance = str(instance).translate(target)\n",
    "    instance = instance.strip().split(',')\n",
    "    dev_tuple_id = list(instance[2::2])\n",
    "    dev_tuple_tfidf = list(instance[3::2])\n",
    "    dev_tuple_list.append(list(zip(dev_tuple_id, dev_tuple_tfidf)))\n",
    "    y_dev.append(convert_feature(instance[0]))\n",
    "# x_dev = np.array(x_dev, dtype = float)\n",
    "# y_dev = np.array(y_dev, dtype = float)\n",
    "\n",
    "#train data matrices for sparse matrix\n",
    "r_count=0\n",
    "i=[]\n",
    "j=[]\n",
    "k=[]\n",
    "for r in tuple_list:\n",
    "    for c in r:\n",
    "        i.append(r_count)\n",
    "        j.append(int(c[0]))\n",
    "        k.append(float(c[1]))\n",
    "    r_count += 1\n",
    "\n",
    "#dev data matrices for sparse matrix\n",
    "r_count_dev=0\n",
    "i_dev=[]\n",
    "j_dev=[]\n",
    "k_dev=[]\n",
    "for r in dev_tuple_list:\n",
    "    for c in r:\n",
    "        i_dev.append(r_count_dev)\n",
    "        j_dev.append(int(c[0]))\n",
    "        k_dev.append(float(c[1]))\n",
    "    r_count_dev += 1\n",
    "\n",
    "# make sparse matrices\n",
    "x_train = coo_matrix((k, (i,j)))\n",
    "x_dev = coo_matrix((k_dev, (i_dev,j_dev)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.406661308148297"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "dclf = DummyClassifier(strategy = \"most_frequent\")\n",
    "dclf.fit(x_train,y_train)\n",
    "dclf_predictions = dclf.predict(x_dev)\n",
    "dclf.score(x_dev,y_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build and train classifier using train data, then output predictions of x_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:985: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\"Liblinear failed to converge, increase \"\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(x_train,y_train)\n",
    "mnb_predictions = mnb.predict(x_dev)\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(x_train,y_train)\n",
    "lr_predictions = lr.predict(x_dev)\n",
    "\n",
    "sclf = LinearSVC()\n",
    "sclf.fit(x_train,y_train)\n",
    "sclf_predictions = sclf.predict(x_dev)\n",
    "\n",
    "# dt = DecisionTreeClassifier(random_state = 0)\n",
    "# dt.fit(x_train,y_train)\n",
    "# dt_predictions = dt.predict(x_dev)\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = 60, random_state = 0)\n",
    "rf.fit(x_train,y_train)\n",
    "rf_predictions = rf.predict(x_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dclf_acc =  0.406661308148297\n",
      "dclf_f1 =  0.19273121198052448\n",
      "mnb_acc =  0.7307846880337586\n",
      "mnb_f1 =  0.7466609947326751\n",
      "sclf_acc =  0.7408319099768914\n",
      "sclf_f1 =  0.7564541742479624\n",
      "rf_acc =  0.6364412739877424\n",
      "rf_f1 =  0.6223057385849048\n",
      "lr_acc =  0.7406812016477444\n",
      "lr_f1 =  0.7563062718161794\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "dclf_acc = accuracy_score(y_dev, dclf_predictions)\n",
    "dclf_f1 = f1_score(y_dev, dclf_predictions, average = 'macro')\n",
    "\n",
    "mnb_acc = accuracy_score(y_dev, mnb_predictions)\n",
    "mnb_f1 = f1_score(y_dev, mnb_predictions, average = 'macro')\n",
    "\n",
    "sclf_acc = accuracy_score(y_dev, sclf_predictions)\n",
    "sclf_f1 = f1_score(y_dev, sclf_predictions, average = 'macro')\n",
    "\n",
    "# dt_acc = accuracy_score(y_dev, dt_predictions)\n",
    "# dt_f1 = f1_score(y_dev, dt_predictions, average = 'macro')\n",
    "\n",
    "rf_acc = accuracy_score(y_dev, rf_predictions)\n",
    "rf_f1 = f1_score(y_dev, rf_predictions, average = 'macro')\n",
    "\n",
    "lr_acc = accuracy_score(y_dev, lr_predictions)\n",
    "lr_f1 = f1_score(y_dev, lr_predictions, average = 'macro')\n",
    "\n",
    "print(\"dclf_acc = \", dclf_acc)\n",
    "print(\"dclf_f1 = \", dclf_f1)\n",
    "print(\"mnb_acc = \", mnb_acc)\n",
    "print(\"mnb_f1 = \", mnb_f1)\n",
    "print(\"sclf_acc = \", sclf_acc)\n",
    "print(\"sclf_f1 = \", sclf_f1)\n",
    "# print(\"dt_acc = \", dt_acc)\n",
    "# print(\"dt_f1 = \", dt_f1)\n",
    "print(\"rf_acc = \", rf_acc)\n",
    "print(\"rf_f1 = \", rf_f1)\n",
    "print(\"lr_acc = \", lr_acc)\n",
    "print(\"lr_f1 = \", lr_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions for Kaggle using prefered classifier (lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tuple_id = []\n",
    "test_tuple_tfidf = []\n",
    "test_tuple_list = []\n",
    "test_tweet_id = []\n",
    "\n",
    "#load test tfidf matrix\n",
    "for instance in test_data[1:]:\n",
    "    instance = str(instance).translate(target)\n",
    "    instance = instance.strip().split(',')\n",
    "    test_tuple_id = list(instance[2::2])\n",
    "    test_tuple_tfidf = list(instance[3::2])\n",
    "    test_tuple_list.append(list(zip(test_tuple_id, test_tuple_tfidf)))\n",
    "    test_tweet_id.append(instance[1])\n",
    "    \n",
    "#test data matrices for sparse matrix\n",
    "r_count_test=0\n",
    "i_test=[]\n",
    "j_test=[]\n",
    "k_test=[]\n",
    "for r in test_tuple_list:\n",
    "    for c in r:\n",
    "        i_test.append(r_count_test)\n",
    "        j_test.append(int(c[0]))\n",
    "        k_test.append(float(c[1]))\n",
    "    r_count_test += 1\n",
    "x_test = coo_matrix((k_test, (i_test,j_test)))\n",
    "\n",
    "clf = lr\n",
    "test_predictions = clf.predict(x_test)\n",
    "\n",
    "def convert_sentiment(raw):\n",
    "    if raw == 2: return \"neg\"\n",
    "    elif raw == 1: return \"neu\"\n",
    "    elif raw == 0: return \"pos\"\n",
    "\n",
    "with open('predictions.csv','w+') as pf:\n",
    "    pf.write('tweet_id,sentiment\\n')\n",
    "    for prediction, tweet_id in zip(test_predictions,test_tweet_id):\n",
    "        pf.write(str(tweet_id) + ',')\n",
    "        pf.write(convert_sentiment(prediction) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8fac594bfae6525c0c41b4041d2d72effa188cc8ead05f81b1fab2bb098927fb"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
